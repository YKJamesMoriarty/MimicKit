agent_name: "ASE"

model:
  actor_net: "fc_3layers_1024units"
  actor_init_output_scale: 0.01
  actor_std_type: "FIXED"
  action_std: 0.05
  
  critic_net: "fc_3layers_1024units"

  disc_net: "fc_3layers_1024units"
  
  enc_net: "fc_2layers_1024units"
  latent_dim: 64

optimizer:
    type: "Adam"
    learning_rate: 2e-5
    disc_learning_rate: 2e-6

discount: 0.99
steps_per_iter: 32
iters_per_output: 1 #记录output到log的频率
test_episodes: 32
normalizer_samples: 500000000

update_epochs: 5
batch_size: 4
td_lambda: 0.95
ppo_clip_ratio: 0.2
norm_adv_clip: 4.0
action_bound_weight: 10.0
action_entropy_weight: 0.0
action_reg_weight: 0.0
critic_loss_weight: 1.0

disc_buffer_size: 50000
disc_replay_samples: 1000
disc_batch_size: 1024
# 判别器 loss 在总 loss 中的权重
disc_loss_weight: 3.0 # 从5.0 降低到 3.0
disc_logit_reg: 0.01 
# 高梯度惩罚（grad_penalty） = 判别器更保守、更稳定，但在判别器已经过强的情况下，这反而让它维持在"过强"的状态
disc_grad_penalty: 3 #从 5 降低为 3（梯度惩罚减弱）
disc_weight_decay: 0.0001
disc_reward_scale: 2

diversity_weight: 0.01
diversity_tar: 1.0

enc_loss_weight: 5.0

latent_time_min: 0.0
# 调整为平均动作长度
latent_time_max: 2.5

task_reward_weight: 0.0
disc_reward_weight: 0.5
enc_reward_weight: 0.5

